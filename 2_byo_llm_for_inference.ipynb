{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "dhlwkl3z4hgjsii53mrm",
   "authorId": "308287954429",
   "authorName": "ADMIN",
   "authorEmail": "prabhath.nanisetty@snowflake.com",
   "sessionId": "22823155-1f87-4730-bec6-39cd661168a9",
   "lastEditTime": 1744906308447
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026d6eb6-2883-49d3-9c69-1de43b87ced6",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "# Bring Your Own LLM Model for Inference\n\nIn this notebook, we will test the Qwen 2.5 - 0.5BN parameter model and later, log it to the Model Registry, and finally create a container service to run inference. You should be able to change the choice of model easily, however, some models require slightly different pipeline flows.\n\nWe have not tested larger versions of Qwen, in order to do this, you may need to modify the `INSTANCE_FAMILY` parameter to reflect the needed GPU resources for larger models. Instance Families across cloud providers are described [here](https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool)."
  },
  {
   "cell_type": "markdown",
   "id": "8d198f7e-1287-4916-b1fc-0acaaca5e437",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Step 1: Test LLM model in this notebook, without creating a service"
  },
  {
   "cell_type": "code",
   "id": "5bd1f550-7958-4471-9ef7-ae4887d22c35",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "# Standard huggingface flow using 'text-generation' pipelines\nfrom transformers import pipeline\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\npipe = pipeline(\"text-generation\", model_name, torch_dtype=\"auto\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "432db265-1260-4d40-9eda-3cbd0231bcac",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "# Create and test a batched input\npipe.tokenizer.padding_side=\"left\"\n\nsystem_message = 'You are a marketing assistant. For each idea, please provide a witty marketing tagline. Please only generate a single tagline and do not provide any other commentary.'\n\nmessage_batch = [\n    [{\"role\": \"user\", \"content\": \"Mobile app for calling a taxi\"}, {\"role\": \"system\", \"content\": system_message}],\n    [{\"role\": \"user\", \"content\": \"Paper towels that have Christmas prints\"}, {\"role\": \"system\", \"content\": system_message}],\n]\n\nresult_batch = pipe(message_batch, max_new_tokens=512, batch_size=2)\nresponse_message_batch = [result[0][\"generated_text\"] for result in result_batch]\nresponse_message_batch",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "698db5cc-f9cb-49c3-844e-8fcfa06315f4",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Step 2: Log LLM model (or fine-tuned version) to Model Registry\n\nModel Registry Documentation: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview\n\n* Standard HuggingFace Pipelines: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/built-in-models/hugging-face - the example below constructs a custommodel\n* Custom Model Pipeline: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/bring-your-own-model-types"
  },
  {
   "cell_type": "code",
   "id": "60c35d32-93fc-469e-9e36-c0b4ef731fa3",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# Build the custom model class\nimport os\nimport torch\nimport pandas as pd\nfrom transformers import pipeline\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import custom_model, model_signature\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Create a custom model class for the instantiation and inference of this model\nclass Qwen2Model(custom_model.CustomModel):\n    def __init__(self, context: custom_model.ModelContext) -> None:\n        super().__init__(context)\n\n        # For `transformers` set the environment variables to use local files only\n        # We will download them to a local dir using huggingface_hub\n        os.environ['HF_HUB_OFFLINE'] = '1'\n        os.environ['TRANSFORMERS_OFFLINE'] = '1'\n        \n        self.pipe = pipeline(\n            \"text-generation\", \n            context.path(\"model_path\"), \n            torch_dtype=\"auto\",\n            device=0,\n        )\n        self.pipe.tokenizer.padding_side=\"left\"\n\n    # Inference function with a dataframe as input\n    @custom_model.inference_api\n    def predict(self, prompt_df: pd.DataFrame) -> pd.DataFrame:\n        prompts = prompt_df['prompts'].tolist()\n\n        messages = [[{\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n\n        results = self.pipe(messages, max_new_tokens=512, batch_size=len(messages))\n        responses = [result[0][\"generated_text\"] for result in results]\n        \n        return pd.DataFrame({\n            \"prompt\": [response[0][\"content\"] for response in responses],\n            \"response\": [response[1][\"content\"] for response in responses]\n        })",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0105df73-7773-4e24-b1e4-1abc9349642e",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Download a model from huggingface to a local directory\n# TO USE YOUR OWN MODEL, skip this step and pass in the model directory path in the place of \n# `local_model_location`. Finally instantiate the CustomModel class.\nimport tempfile\nfrom huggingface_hub import snapshot_download\n\ntmpdir = tempfile.mkdtemp()\nlocal_model_location = snapshot_download(\n    repo_id=model_name,\n    local_dir=tmpdir\n)\n\npath_list = {\"model_path\": local_model_location}\nqwen = Qwen2Model(context=custom_model.ModelContext(artifacts=path_list))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9f368f2-9dfa-4d75-89bd-6c583c3415c0",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# Generate a reponse from the model using the predict() method\ntest_prompt = pd.DataFrame(['What is the internet?', 'The capital of France is'], columns=['prompts'])\n\nresponse = qwen.predict(test_prompt)\nresponse",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "833cd73c-e1bd-4484-9482-01ddeea0e4a5",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Infer the model signature from the input prompts and the response above.\n# Documentation: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-signature\nsignature = model_signature.infer_signature(test_prompt, response)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e2dbbb7-3725-40e8-9dfe-181b18de0cb8",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Log the model to the Snowflake Model Registry\nreg = Registry(session)\nmv = reg.log_model(\n    qwen,\n    model_name='QWEN25',\n    version_name='V4',  # Can remove this parameter to auto-create version names\n    conda_dependencies=['transformers', 'tokenizers', 'pytorch', 'huggingface_hub', 'snowflake-ml-python'],\n    signatures={\"predict\":signature},\n    options={\"cuda_version\": \"11.8\"}\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67e14500-bed5-4db8-98b2-6355d9792ff9",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# This step SHOULD fail!!\n# The default for models is to predict using a warehouse, however, these models will need GPU inferencing\nmv.run(test_prompt)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5abd8861-e21c-44d3-b971-2bb683d9fda9",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "## 3. Create a Container Service for Model Serving\n\nRead more here: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container\n\n*>>> Important Note: This is a long-running service, so once you are done, you will want to suspend the service to stop incurring costs. To do this, run `ALTER SERVICE QWEN_SERVICE SUSPEND;` in a Notebook or SQL worksheet*"
  },
  {
   "cell_type": "code",
   "id": "c7e2c805-60e5-4d77-8ebd-7e1535f5cec0",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# Create a compute pool for GPU access to run this service\n\n# Compute Pool definition\nDATABASE_NAME = 'NOTEBOOK_DEMO_DB'\nSCHEMA_NAME = 'LLM_TEST_QWEN'\nIMAGE_REPO_NAME = \"QWEN_SERVICE_REPO\"\nCOMPUTE_POOL_NAME = \"QWEN_SERVICE_POOL_S\"\nCOMPUTE_POOL_NODES = 1\nCOMPUTE_POOL_INSTANCE_TYPE = 'GPU_NV_S'\n\nsession.sql(f\"use database {DATABASE_NAME};\").collect()\nsession.sql(f\"use schema {SCHEMA_NAME};\").collect()\nsession.sql(f\"create image repository if not exists {IMAGE_REPO_NAME}\").collect()\nsession.sql(f\"alter compute pool if exists {COMPUTE_POOL_NAME} stop all\").collect()\nsession.sql(f\"drop compute pool if exists {COMPUTE_POOL_NAME}\").collect()\nsession.sql(f\"create compute pool if not exists {COMPUTE_POOL_NAME} min_nodes={COMPUTE_POOL_NODES} \" +\n            f\"max_nodes={COMPUTE_POOL_NODES} instance_family={COMPUTE_POOL_INSTANCE_TYPE} \" +\n            f\"initially_suspended=True auto_resume=True auto_suspend_secs=300\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5f85f0d-980e-49bb-95c6-2ffd38739fc7",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create a Service object that can be called easily\n# Name of the Service for powering inference\nSERVICE_NAME = 'QWEN_SERVICE'\n\n# **This step may take >15 mins** - it is building a full container runtime.\nmv.create_service(\n    service_name=SERVICE_NAME,\n    service_compute_pool=COMPUTE_POOL_NAME,\n    image_repo=IMAGE_REPO_NAME,\n    gpu_requests='1',\n    ingress_enabled=True,\n    max_instances=int(COMPUTE_POOL_NODES),\n    build_external_access_integration='ALLOW_ALL_INTEGRATION'\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f68eb8b8-2fa9-4634-adf3-df5fea7fa839",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "## 4. Serve model from Registry and use for Inference\nThis code can be used in other places like a streamlit app or from a SQL worksheet to call the LLM model\n\nDocumentation link: https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container#using-a-model-deployed-to-spcs"
  },
  {
   "cell_type": "code",
   "id": "f204c32e-85ac-4c96-8ba4-789e94ac19dc",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "# PYTHON CALL - useful for Streamlit app\n# Pull Model from Registry for Inference\nfrom snowflake.ml.registry import Registry\nfrom snowflake.snowpark.context import get_active_session\n\n# Modify these based on your details.\nDATABASE_NAME = 'NOTEBOOK_DEMO_DB'\nSCHEMA_NAME = 'LLM_TEST_QWEN'\nSELECTED_MODEL = 'QWEN25'\nMODEL_VERSION = 'V4'\n\nsession = get_active_session()\nreg = Registry(session=session, database_name=DATABASE_NAME, schema_name=SCHEMA_NAME)\nqwen_from_registry = reg.get_model(SELECTED_MODEL).version(MODEL_VERSION)\n\nqwen_from_registry.run(test_prompt, service_name=SERVICE_NAME)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "868a09b8-69ba-494f-bd69-4c9359c8b54c",
   "metadata": {
    "language": "sql",
    "name": "cell15"
   },
   "outputs": [],
   "source": "-- SQL CALL - useful for applying to a table of data\n-- Note: in the customModel class, you may want to modify the predict function to accept more than\n-- one row of question/answer in order to be more performant when applied to a table of data\nUSE DATABASE NOTEBOOK_DEMO_DB;\nUSE SCHEMA LLM_TEST_QWEN;\nSELECT QWEN_SERVICE!PREDICT('What is are large language models?');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83f70cad-f84b-4e47-bbd0-bfb65dced16c",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE LOCAL TEMPORARY TABLE IDEA_GENS ON COMMIT PRESERVE ROWS AS\n    SELECT\n        SNOWFLAKE.CORTEX.COMPLETE(\n            'llama3.1-70b',\n            [\n                {\n                    'role': 'user',\n                    'content': 'Please give me the name of a country at random. Don''t include any extra commentary, only the name of a country.'\n                }\n            ],\n            {'temperature': 0.7}\n        ) AS IDEA_TEXT\n    FROM TABLE(GENERATOR(ROWCOUNT => 1000)) t;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12ac4d07-54a1-48fe-8df3-543080b0c9fb",
   "metadata": {
    "language": "sql",
    "name": "cell19"
   },
   "outputs": [],
   "source": "-- Test using our model against all 1,000 rows\nALTER SESSION SET QUERY_TAG = 'llm_vectorization_test';\nSELECT\n    IDEA_TEXT,\n    QWEN_SERVICE!PREDICT(\n        CONCAT(\n            'What is the capital of this country? Only provide the name of the capital: ',\n            IDEA_TEXT:choices[0].messages::VARCHAR\n        )\n    ) as marketing_idea\nFROM IDEA_GENS;",
   "execution_count": null
  }
 ]
}